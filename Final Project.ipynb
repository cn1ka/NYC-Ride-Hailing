{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "\n",
    "import os\n",
    "import re\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pyarrow.parquet as pq\n",
    "import geopandas as gpd\n",
    "import warnings\n",
    "from typing import List, Tuple, Optional\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import folium\n",
    "from folium.plugins import HeatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TLC_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        #the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f90aa3",
   "metadata": {},
   "source": [
    "### Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9524852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html() -> bytes:\n",
    "    \"\"\"\n",
    "    Request the HTML content of TLC_URL webpage.\n",
    "\n",
    "    Returns:\n",
    "        bytes: The HTML content of the webpage in bytes type.\n",
    "    \"\"\"\n",
    "    response = requests.get(TLC_URL)\n",
    "    html = response.content\n",
    "    \n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d825e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_parquet_links()-> List[str]:\n",
    "    \"\"\"\n",
    "    Finds and returns a list of URLs of Yellow Taxi parquet files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: a list of strings representing URLs of Yellow Taxi \n",
    "        parquet files.\n",
    "    \"\"\"\n",
    "    parquet_links = list()\n",
    "    \n",
    "    html = get_html()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    #using re module to help me extract links for Yellow Taxi \n",
    "    pattern = re.compile(r\"Yellow Taxi Trip Records\")\n",
    "    for link in links:\n",
    "        title = link.get('title')\n",
    "        if title != None:\n",
    "            match = pattern.search(title)\n",
    "            if match:\n",
    "                #collect URLs of Yellow Taxi parquet files\n",
    "                parquet_links.append(link.get('href'))\n",
    "                \n",
    "    return parquet_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea2c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_HVFHV_parquet_links() -> List[str]:\n",
    "    \"\"\"\n",
    "    Finds and returns a list of URLs of High Volume FHV parquet files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: a list of strings representing URLs of High Volume FHV \n",
    "            parquet files.\n",
    "    \"\"\"\n",
    "    parquet_links = list()\n",
    "    \n",
    "    html = get_html()\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    #using re module to extract links for High Volume For-Hire Vehicle \n",
    "    pattern = re.compile(r\"High Volume For-Hire Vehicle Trip Records\")\n",
    "    for link in links:\n",
    "        title = link.get('title')\n",
    "        if title != None:\n",
    "            match = pattern.search(title)\n",
    "            if match:\n",
    "                #collect URLs of HVFHV parquet files\n",
    "                parquet_links.append(link.get('href'))\n",
    "                \n",
    "    return parquet_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf07184",
   "metadata": {},
   "source": [
    "### Downloading Taxi files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cf0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_taxi_parquet_files():\n",
    "    \"\"\"\n",
    "    Downloads Yellow Taxi parquet files from URLs found by 'find_taxi_\n",
    "    parquet_links()'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    taxi_files = find_taxi_parquet_links()\n",
    "    for file_url in taxi_files:\n",
    "        file_url = file_url.replace(' ', '')\n",
    "        name = file_url.split('trip-data/')[1]\n",
    "        #check if the file already exists\n",
    "        if os.path.exists(name):\n",
    "            pass\n",
    "        else:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            with open(name, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024): \n",
    "                    if chunk:\n",
    "                        f.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69dcea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function to download yellow_taxi_files programmatically\n",
    "#download_taxi_parquet_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d14df8",
   "metadata": {},
   "source": [
    "### Downloading HVFHV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f7b8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_HVFHV_parquet_files() -> None:\n",
    "    \"\"\"\n",
    "    Downloads High Volume FHV parquet files from URLs found by 'find_\n",
    "    HVFHV_parquet_links'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    taxi_files = find_HVFHV_parquet_links()\n",
    "    for file_url in taxi_files:\n",
    "        file_url = file_url.replace(' ', '')\n",
    "        name = file_url.split('trip-data/')[1]\n",
    "        #check if the file already exists\n",
    "        if os.path.exists(name):\n",
    "            pass\n",
    "        else:\n",
    "            response = requests.get(file_url, stream=True)\n",
    "            with open(name, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024): \n",
    "                    if chunk:\n",
    "                        f.write(chunk)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function to download HVFHV_parquet_files programmatically\n",
    "#download_HVFHV_parquet_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc8135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile: str) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Read a shapefile containing taxi zones into a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        shapefile(str):\n",
    "            The path to the shapefile that contains the taxi zones data.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: A GeoDataFrame that contains the taxi zones data.\n",
    "    \"\"\"\n",
    "    data = gpd.read_file(shapefile)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and store taxi zones data\n",
    "loaded_taxi_zones = load_taxi_zones('taxi_zones.shp')\n",
    "loaded_taxi_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id: int, loaded_taxi_zones: gpd.GeoDataFrame) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Lookup the coordinates of for a given taxi zone Location ID.\n",
    "\n",
    "    Parameters:\n",
    "        zone_loc_id(int): The location ID for the taxi zone\n",
    "        loaded_taxi_zones(gpd.GeoDataFrame):A GeoDataFrame that contains taxi zone geometries.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: A tuple containing the longitude and latitude \n",
    "        of the given location ID. \n",
    "        Returns (None, None) if the ID is not found.\n",
    "    \"\"\"\n",
    "    row = loaded_taxi_zones[loaded_taxi_zones['LocationID']==zone_loc_id]\n",
    "    if (len(row)!=0):\n",
    "        #coordinate reference system 4326\n",
    "        row = row.to_crs(4326)\n",
    "        #extract centroid\n",
    "        centroid = row.geometry.centroid.iloc[0]\n",
    "\n",
    "        return centroid.x, centroid.y\n",
    "    else:\n",
    "        \n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sample_size(p: float, e: float, z: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the sample size using Cochran's Formula.\n",
    "\n",
    "    Parameters:\n",
    "        p (float): the (estimated) proportion of the population.\n",
    "        e (float): the desired level of precision.\n",
    "        z (float): Z values based on the confidence level.\n",
    "\n",
    "    Returns:\n",
    "        float: The sample size calculated by Cochran's Formula.\n",
    "    \"\"\"\n",
    "    n0 = ((z**2)*p*(1-p))/(e**2)\n",
    "    \n",
    "    return n0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_month(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract a sample of taxi data from the dataset of given url and clean it.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the Parquet file containing the Yellow Taxi data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame containing the relevant columns and data.\n",
    "    \"\"\"\n",
    "\n",
    "    #get file's name from url\n",
    "    file_url = url.replace(' ', '')\n",
    "    name = file_url.split('trip-data/')[1]\n",
    "\n",
    "    #only extract a specific sample size of data\n",
    "    sample = int(calculate_sample_size(p=0.5, e=0.05, z=1.96))\n",
    "    #read a sample of data by calling its name\n",
    "    data = pq.read_table(name).to_pandas().sample(n=sample, random_state=1)\n",
    "    \n",
    "    #extract specific columns we want (there are three types of dataframes)\n",
    "\n",
    "    #the first type of dataset\n",
    "    if ('VendorID' in data.columns):\n",
    "        #clean values in relevant columns before adding them up as a total surcharge\n",
    "        data = data.rename(columns={'airport_fee': 'Airport_fee', 'Airport_Fee': 'Airport_fee'})\n",
    "        data['Airport_fee'] = data['Airport_fee'].fillna(0)\n",
    "        data = data.dropna(subset=['improvement_surcharge', 'congestion_surcharge', 'extra', 'tip_amount'], how='any')\n",
    "        data = data[~(data[['improvement_surcharge', 'congestion_surcharge', 'extra', 'tip_amount', 'Airport_fee']]< 0).any(axis=1)]\n",
    "        \n",
    "        #define new columns for storing information(coordinates and surcharge)\n",
    "        data['pickup_longitude'] = None\n",
    "        data['pickup_latitude'] = None\n",
    "        data['dropoff_longitude'] = None\n",
    "        data['dropoff_latitude'] = None\n",
    "        data['surcharge'] = None\n",
    "        # use shp to find lat and lon for pickup/dropoff location\n",
    "        drop_index = list()\n",
    "        for index, row in data.iterrows():\n",
    "            lon, lat = lookup_coords_for_taxi_zone_id(row['PULocationID'],loaded_taxi_zones)\n",
    "            lon2, lat2 = lookup_coords_for_taxi_zone_id(row['DOLocationID'],loaded_taxi_zones)\n",
    "            if (lon != None and lat != None and lon2 != None and lat2 != None):\n",
    "                data.loc[index,'pickup_longitude'] = lon\n",
    "                data.loc[index,'pickup_latitude'] = lat\n",
    "                data.loc[index,'dropoff_longitude'] = lon2\n",
    "                data.loc[index,'dropoff_latitude'] = lat2\n",
    "            else:\n",
    "                #store rows where pickup locationID or dropoff locationID is not valid\n",
    "                drop_index.append(index)\n",
    "            #calculate total surcharge of each taxi ride\n",
    "            data.loc[index,'surcharge'] = row['improvement_surcharge'] + row['congestion_surcharge'] + row['extra'] + row['tip_amount'] + row['Airport_fee']\n",
    "        #drop rows where pickup locationID or dropoff locationID is not valid    \n",
    "        data = data.drop(drop_index)\n",
    "        #only keep columns needed \n",
    "        data = data[['VendorID','tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance','pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude','fare_amount', 'surcharge', 'mta_tax', 'tolls_amount', 'tip_amount']]       \n",
    "\n",
    "    #the second type of dataset\n",
    "    if ('vendor_id' in data.columns):\n",
    "        #clean values in relevant columns before adding them up as a total surcharge\n",
    "        data = data.dropna(subset=['surcharge', 'tip_amount'], how='any')\n",
    "        #calculate total surcharge of each taxi ride   \n",
    "        data['surcharge2'] = data['surcharge'] + data['tip_amount']\n",
    "        #only keep columns needed \n",
    "        data = data[['vendor_id','pickup_datetime','dropoff_datetime', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'surcharge2', 'mta_tax', 'tolls_amount', 'tip_amount']]\n",
    "\n",
    "    #the third type of dataset\n",
    "    if ('vendor_name' in data.columns):\n",
    "        #clean values in relevant columns before adding them up as a total surcharge\n",
    "        data = data.dropna(subset=['surcharge', 'Tip_Amt'], how='any')  \n",
    "        #calculate total surcharge of each taxi ride    \n",
    "        data['surcharge2'] = data['Tip_Amt'] + data['surcharge']\n",
    "        #only keep columns needed \n",
    "        data = data[['vendor_name','Trip_Pickup_DateTime','Trip_Dropoff_DateTime', 'Trip_Distance', 'Start_Lon', 'Start_Lat', 'End_Lon', 'End_Lat', 'Fare_Amt', 'surcharge2', 'mta_tax', 'Tolls_Amt', 'Tip_Amt']]\n",
    "\n",
    "    #normalize column names\n",
    "    new_column_names = ['VendorID', 'pickup_datetime', 'dropoff_datetime', 'trip_distance', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'fare_amount', 'surcharge', 'taxes', 'tolls_amount', 'tip_amount']\n",
    "    data.columns = new_column_names\n",
    "\n",
    "    #normalize appropriate column types\n",
    "    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
    "    data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])\n",
    "    data['surcharge'] = pd.to_numeric(data['surcharge'])\n",
    "\n",
    "    #removing trips that start and/or end outside (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "    data = data[(data['pickup_latitude'] >= 40.560445) & (data['pickup_latitude'] <= 40.908524) &\n",
    "                (data['pickup_longitude'] >= -74.242330) & (data['pickup_longitude'] <= -73.717047) &\n",
    "                (data['dropoff_latitude'] >= 40.560445) & (data['dropoff_latitude'] <= 40.908524) &\n",
    "                (data['dropoff_longitude'] >= -74.242330) & (data['dropoff_longitude'] <= -73.717047)]\n",
    "\n",
    "    #removing a distance of 0\n",
    "    drop_index2 = list()\n",
    "    for index, row in data.iterrows():\n",
    "        #drop rows with same pickup and dropoff location\n",
    "        if ((row['pickup_longitude']==row['dropoff_longitude']) and (row['pickup_latitude']==row['dropoff_latitude'])):\n",
    "            drop_index2.append(index)\n",
    "    data = data.drop(drop_index2)\n",
    "    #drop rows with distance of 0 \n",
    "    data = data[data['trip_distance']>0]\n",
    "\n",
    "    #removing invalid data points (NaN)\n",
    "    data = data.dropna()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls: List[str])-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and clean data from a list of URLs for Yellow Taxi Parquet files, and \n",
    "    combine them into one large DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        parquet_urls (list): A list of URLs of Parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all cleaned taxi data.\n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    #read and clean data from every month\n",
    "    for parquet_url in parquet_urls:\n",
    "        #only consider files for yellow taxi\n",
    "        if ('yellow' in parquet_url):\n",
    "            dataframe = get_and_clean_taxi_month(parquet_url)\n",
    "            all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find the links for all taxi data Parquet files, cleans the data, and returns \n",
    "    the DataFrame containing all the taxi data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing all the cleaned yellow-taxi data.\n",
    "    \"\"\"\n",
    "\n",
    "    all_taxi_urls = find_taxi_parquet_links()\n",
    "    taxi_data = get_and_clean_taxi_data(all_taxi_urls)\n",
    "\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cleaned and complete taxi data\n",
    "warnings.filterwarnings('ignore')\n",
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da7089-3f6b-4f93-a22e-76bf554daca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c85e25-6416-4c16-b98c-09596cdc6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07574983-f41d-4cd6-8f70-489493089b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_month(url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Processes, and cleans Uber trip data for a given month.It downloads Uber \n",
    "    trip data from the specified URL, filters the data, normalizes columns, \n",
    "    and removes invalid or out-of-bound data. \n",
    "\n",
    "    Parameters:\n",
    "        url (str): The URL of the Uber trip data file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame containing filtered and normalized Uber trip data.\n",
    "    \"\"\"\n",
    "    \n",
    "    file_url = url.replace(' ', '')\n",
    "    name = file_url.split('trip-data/')[1]\n",
    "    data = pq.read_table(name).to_pandas()\n",
    "\n",
    "    #filter for Uber license\n",
    "    data = data[data['hvfhs_license_num'] == \"HV0003\"]\n",
    "\n",
    "    #sample the data\n",
    "    sample = int(calculate_sample_size(p=0.5, e=0.05, z=1.96))\n",
    "    data = data.sample(n=sample, random_state=1)\n",
    "\n",
    "    #add geographic coordinates for pickup and dropoff locations\n",
    "    if 'hvfhs_license_num' in data.columns:\n",
    "        coords = data.apply(\n",
    "            lambda row: pd.Series(\n",
    "                lookup_coords_for_taxi_zone_id(row['PULocationID'], loaded_taxi_zones) +\n",
    "                lookup_coords_for_taxi_zone_id(row['DOLocationID'], loaded_taxi_zones)\n",
    "            )\n",
    "            if (row['PULocationID'] in loaded_taxi_zones['LocationID'] and\n",
    "                row['DOLocationID'] in loaded_taxi_zones['LocationID'])\n",
    "            else pd.Series([None, None, None, None]),\n",
    "            axis=1\n",
    "    )\n",
    "    coords.columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "    data = pd.concat([data, coords], axis=1)\n",
    "    data = data.dropna(subset=['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'])\n",
    "\n",
    "    #select columns\n",
    "    data = data[[\n",
    "             'request_datetime',\n",
    "             'pickup_datetime',\n",
    "             'dropoff_datetime',\n",
    "             'trip_miles',\n",
    "             'trip_time',\n",
    "             'pickup_longitude',\n",
    "             'pickup_latitude',\n",
    "             'dropoff_longitude',\n",
    "             'dropoff_latitude',\n",
    "             'base_passenger_fare',\n",
    "             'tolls',\n",
    "             'bcf',\n",
    "             'sales_tax',\n",
    "             'congestion_surcharge',\n",
    "             'airport_fee',\n",
    "             'tips'\n",
    "                ]\n",
    "            ] \n",
    "\n",
    "    #normalize appropriate column types - time\n",
    "    data['request_datetime'] = pd.to_datetime(data['request_datetime'])\n",
    "    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n",
    "    data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])\n",
    "    \n",
    "    #normalize appropriate column types - number\n",
    "    data['trip_miles'] = pd.to_numeric(data['trip_miles'])\n",
    "    data['trip_time'] = pd.to_numeric(data['trip_time'])\n",
    "    data['base_passenger_fare'] = pd.to_numeric(data['base_passenger_fare'])\n",
    "    data['tolls'] = pd.to_numeric(data['tolls'])\n",
    "    data['bcf'] = pd.to_numeric(data['bcf'])\n",
    "    data['sales_tax'] = pd.to_numeric(data['sales_tax'])\n",
    "    data['congestion_surcharge'] = pd.to_numeric(data['congestion_surcharge'])\n",
    "    data['airport_fee'] = pd.to_numeric(data['airport_fee'])\n",
    "    data['tips'] = pd.to_numeric(data['tips'])\n",
    "    \n",
    "    #clean relevant columns before adding them up as a total surcharge\n",
    "    data['airport_fee'] = data['airport_fee'].fillna(0)\n",
    "    data = data.dropna(subset=['airport_fee', 'congestion_surcharge', 'bcf', 'tips'], how='any')\n",
    "    #compute total surcharge\n",
    "    data['all_surcharge'] = data['congestion_surcharge'] + data['airport_fee'] + data['bcf'] + data['tips']\n",
    "    \n",
    "    #remove NaN\n",
    "    data = data.dropna()\n",
    "    \n",
    "    #normalize column names (pu, do, trip_time, bcf)\n",
    "    new_column_names = [\n",
    "             'request_datetime',\n",
    "             'pickup_datetime',\n",
    "             'dropoff_datetime',\n",
    "             'trip_miles',\n",
    "             'trip_time_seconds',\n",
    "             'pickup_longitude',\n",
    "             'pickup_latitude',\n",
    "             'dropoff_longitude',\n",
    "             'dropoff_latitude',\n",
    "             'base_passenger_fare',\n",
    "             'tolls',\n",
    "             'black_car_fund',\n",
    "             'sales_tax',\n",
    "             'congestion_surcharge',\n",
    "             'airport_fee',\n",
    "             'tips',\n",
    "             'all_surcharge'\n",
    "                       ]\n",
    "    data.columns = new_column_names\n",
    "\n",
    "    #remove invalid time\n",
    "    data = data[data['pickup_datetime'] < data['dropoff_datetime']]\n",
    "    #remove 0 distance\n",
    "    data = data[data['trip_miles'] > 0]\n",
    "    #remove 0 time\n",
    "    data = data[data['trip_time_seconds'] > 0]\n",
    "    \n",
    "    #removing trips that start and/or end outside (40.560445, -74.242330) and (40.908524, -73.717047)\n",
    "    data = data[(data['pickup_latitude'] >= 40.560445) & (data['pickup_latitude'] <= 40.908524) &\n",
    "                (data['pickup_longitude'] >= -74.242330) & (data['pickup_longitude'] <= -73.717047) &\n",
    "                (data['dropoff_latitude'] >= 40.560445) & (data['dropoff_latitude'] <= 40.908524) &\n",
    "                (data['dropoff_longitude'] >= -74.242330) & (data['dropoff_longitude'] <= -73.717047)]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_uber_data(parquet_urls: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load and clean data from a list of URLs for fhvhv files, and combine them \n",
    "    into one large DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        parquet_urls (List[str]): A list of URLs for Parquet files containing Uber data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned and combined DataFrame containing Uber trip data \n",
    "        from all specified Parquet files.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_uber_dataframes = []\n",
    "    \n",
    "    #process each URL in the list\n",
    "    for parquet_url in parquet_urls:\n",
    "        #only consider fhvhv data\n",
    "        if ('fhvhv' in parquet_url):\n",
    "            dataframe = get_and_clean_uber_month(parquet_url)\n",
    "            all_uber_dataframes.append(dataframe)\n",
    "        \n",
    "    uber_data = pd.concat(all_uber_dataframes)\n",
    "    \n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves all relevant uber URLs from the TLC page, and combined all cleaned datas as one DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame containing Uber trip data from all relevant Parquet files.\n",
    "    \"\"\"\n",
    "    all_uber_urls = find_HVFHV_parquet_links()\n",
    "    uber_data = get_and_clean_uber_data(all_uber_urls)\n",
    "    return uber_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d783db-e527-4847-bf70-2d7428ea3897",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddeb14-cd70-4e83-8f93-974642c3bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs() -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Reads weather data for the years 2020 to 2024 and returns a list of \n",
    "    DataFrames for each year.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of DataFrames containing weather data for each year.\n",
    "    \"\"\"\n",
    "    all_weather_files = list()\n",
    "    #iterate each year from 2020 to 2024\n",
    "    for i in range(2020,2025,1):\n",
    "        #read weather data from specific year\n",
    "        weather_file = pd.read_csv(f'{i}'+'_weather.csv')\n",
    "        all_weather_files.append(weather_file)\n",
    "    \n",
    "    return all_weather_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans weather data by extracting the hour from the 'DATE' column, fixing invalid values in \n",
    "    'HourlyPrecipitation' and 'HourlyWindSpeed', and return the cleaned data in DataFrame.\n",
    "\n",
    "    Parameter:\n",
    "        csv_file (pd.DataFrame): The input file of date containing weather information.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with 'DATE', 'hour', 'HourlyPrecipitation', and 'HourlyWindSpeed'.\n",
    "    \"\"\"\n",
    "\n",
    "    hour_data = csv_file.copy()\n",
    "    #convert date in datetime foramt and extract hour\n",
    "    hour_data['DATE'] = pd.to_datetime(hour_data['DATE'])\n",
    "    hour_data['hour'] = hour_data['DATE'].dt.hour\n",
    "    hour_data['DATE'] = hour_data['DATE'].dt.strftime('%Y-%m-%d')\n",
    "    hour_data['DATE'] = pd.to_datetime(hour_data['DATE'])\n",
    "    \n",
    "    #fix invalid values for HourlyPrecipitation\n",
    "    hour_data['HourlyPrecipitation'] = hour_data['HourlyPrecipitation'].replace('T', 0)\n",
    "    hour_data['HourlyPrecipitation'] = hour_data['HourlyPrecipitation'].str.replace('s', '')\n",
    "    hour_data['HourlyPrecipitation'] = hour_data['HourlyPrecipitation'].fillna(0)\n",
    "    hour_data['HourlyPrecipitation'] = pd.to_numeric(hour_data['HourlyPrecipitation'])\n",
    "    #fix invalid values for HourlyWindSpeed\n",
    "    hour_data['HourlyWindSpeed'] = hour_data['HourlyWindSpeed'].fillna(0)\n",
    "    hour_data['HourlyWindSpeed'] = pd.to_numeric(hour_data['HourlyWindSpeed'])\n",
    "    \n",
    "    hour_data = hour_data[['DATE','hour','HourlyPrecipitation','HourlyWindSpeed']].reset_index(drop=True)\n",
    "    \n",
    "    return hour_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans daily weather data by extracting daily record time, fixing invalid values, and\n",
    "    returning a cleaned DataFrame with relevant columns.\n",
    "\n",
    "    Parameters:\n",
    "        csv_file (pd.DataFrame): The input DataFrame containing weather data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A cleaned DataFrame with 'DATE', 'DailyPrecipitation', \n",
    "        'DailyAverageWindSpeed', and 'DailySnowfall'.\n",
    "    \"\"\"\n",
    "    daily_data = csv_file.copy()\n",
    "    #convert 'DATE' to datetime format and extract weather information when time is 23:59\n",
    "    daily_data['DATE'] = pd.to_datetime(daily_data['DATE'])\n",
    "    daily_data = daily_data[daily_data['DATE'].dt.strftime('%H:%M') == '23:59']\n",
    "    daily_data['DATE'] = daily_data['DATE'].dt.strftime('%Y-%m-%d')\n",
    "    daily_data = daily_data[~(daily_data.duplicated(['DATE'], keep=False) & (daily_data['DailyPrecipitation'].isna()) & (daily_data['DailyAverageWindSpeed'].isna()) & (daily_data['DailySnowfall'].isna()))]\n",
    "    daily_data['DATE'] = pd.to_datetime(daily_data['DATE'])\n",
    "    \n",
    "    #fix invalid values for DailyPrecipitation\n",
    "    daily_data['DailyPrecipitation'] = daily_data['DailyPrecipitation'].replace('T', 0)\n",
    "    daily_data['DailyPrecipitation'] = daily_data['DailyPrecipitation'].fillna(0)\n",
    "    daily_data['DailyPrecipitation'] = pd.to_numeric(daily_data['DailyPrecipitation'])\n",
    "\n",
    "    #fix invalid values for DailyAverageWindSpeed\n",
    "    daily_data['DailyAverageWindSpeed'] = daily_data['DailyAverageWindSpeed'].fillna(0)\n",
    "    daily_data['DailyAverageWindSpeed'] = pd.to_numeric(daily_data['DailyAverageWindSpeed'])\n",
    "\n",
    "    #fix invalid values for DailyAverageWindSpeed\n",
    "    daily_data['DailySnowfall'] = daily_data['DailySnowfall'].replace('T', 0)\n",
    "    daily_data['DailySnowfall'] = daily_data['DailySnowfall'].fillna(0)\n",
    "    daily_data['DailySnowfall'] = pd.to_numeric(daily_data['DailySnowfall'])\n",
    "    \n",
    "    daily_data = daily_data[['DATE','DailyPrecipitation','DailyAverageWindSpeed','DailySnowfall']].reset_index(drop=True)\n",
    "    \n",
    "    return daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Loads and cleans weather data into hourly and daily dataframes from CSV files.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing hourly_data and daily_data\n",
    "    \"\"\"\n",
    "\n",
    "    weather_csv_files = get_all_weather_csvs()\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    #receives hourly weather data and daily weather data for each year\n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes that combines all hourly or daily data \n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935261b7-ae23-427c-97ff-ea31aa4e44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dcb502-d1d1-447d-aa68-11bff0dc53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f090eb94-a5b0-4d93-bf82-a596d2521b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c074aa3-a5f2-4586-8748-411e1e6c11da",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    " #use SQLite3 to create 4 tables\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "Create Table IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATETIME,\n",
    "    hour INTEGER,\n",
    "    HourlyPrecipitation REAL,\n",
    "    HourlyWindSpeed REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "Create Table IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATETIME,\n",
    "    DailyPrecipitation REAL,\n",
    "    DailyAverageWindSpeed REAL,\n",
    "    DailySnowfall REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATETIME,\n",
    "    dropoff_datetime DATETIME,\n",
    "    trip_distance REAL,\n",
    "    pickup_longitude REAL,\n",
    "    pickup_latitude REAL,\n",
    "    dropoff_longitude REAL,\n",
    "    dropoff_latitude REAL,\n",
    "    fare_amount REAL,\n",
    "    surcharge REAL,\n",
    "    taxes REAL,\n",
    "    tolls_amount REAL,\n",
    "    tip_amount REAL\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "Create Table IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    request_datetime DATETIME,\n",
    "    driver_arrived_datetime DATETIME,\n",
    "    pickup_datetime DATETIME,\n",
    "    dropoff_datetime DATETIME,\n",
    "    trip_miles REAL,\n",
    "    trip_time_seconds INTEGER,\n",
    "    pickup_longitude REAL,\n",
    "    pickup_latitude REAL,\n",
    "    dropoff_longitude REAL,\n",
    "    dropoff_latitude REAL,\n",
    "    base_passenger_fare REAL,\n",
    "    tolls REAL,\n",
    "    black_car_fund REAL,\n",
    "    sales_tax REAL,\n",
    "    congestion_surcharge REAL,\n",
    "    airport_fee REAL,\n",
    "    tips REAL,\n",
    "    all_surcharge REAL\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the tables with the schema files\n",
    "with open(DATABASE_SCHEMA_FILE, 'r') as f:\n",
    "    schema_sql = f.read()\n",
    "    sql_statement = schema_sql.split(';')\n",
    "        \n",
    "#execute the SQL commands to create the tables\n",
    "with connection:\n",
    "    for statement in sql_statement:\n",
    "        connection.execute(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict: Dict[str, pd.DataFrame], connection: sqlite3.Connection) -> None:\n",
    "    \"\"\"\n",
    "    Inserts rows from the provided DataFrames into their corresponding database tables.\n",
    "\n",
    "    Args:\n",
    "        table_to_df_dict (Dict[str, pd.DataFrame]): \n",
    "            A dictionary where the keys are table names and the values are pandas DataFrames \n",
    "            containing data to insert into the respective tables.\n",
    "        connection (sqlite3.Connection): \n",
    "            A connection object for the SQLite database.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Notes:\n",
    "        This function handles \"taxi_trips\" table, \"hourly_weather\" table, \n",
    "        \"daily_weather\" table, and \"uber_trips\" table.\n",
    "    \"\"\"\n",
    "    for table_name in table_to_df_dict:\n",
    "        #add data to Taxi database\n",
    "        if (table_name == 'taxi_trips'):\n",
    "            df = table_to_df_dict[table_name]\n",
    "            insert_query = \"\"\"\n",
    "            INSERT INTO taxi_trips (pickup_datetime, dropoff_datetime, \n",
    "                                    trip_distance,\n",
    "                                    pickup_longitude, pickup_latitude, \n",
    "                                    dropoff_longitude, dropoff_latitude,\n",
    "                                    fare_amount, surcharge, taxes, tolls_amount, tip_amount)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "    \n",
    "            #loop through DataFrame rows and insert each row into the table\n",
    "            data = []\n",
    "            for index, row in df.iterrows():\n",
    "                pickup_datetime = row['pickup_datetime'].strftime('%Y-%m-%d %H:%M:%S') \n",
    "                dropoff_datetime = row['dropoff_datetime'].strftime('%Y-%m-%d %H:%M:%S') \n",
    "                data.append((\n",
    "                pickup_datetime, dropoff_datetime,\n",
    "                row['trip_distance'],\n",
    "                row['pickup_longitude'], row['pickup_latitude'],\n",
    "                row['dropoff_longitude'], row['dropoff_latitude'],\n",
    "                row['fare_amount'], row['surcharge'], row['taxes'], row['tolls_amount'], row['tip_amount']\n",
    "                ))\n",
    "        \n",
    "            with connection:\n",
    "                connection.executemany(insert_query,data)\n",
    "\n",
    "        #add data to hourly weather database  \n",
    "        if (table_name == 'hourly_weather'):\n",
    "            df = table_to_df_dict[table_name]\n",
    "            insert_query2 = \"\"\"\n",
    "            INSERT INTO hourly_weather(DATE, hour,\n",
    "                                       HourlyPrecipitation,\n",
    "                                       HourlyWindSpeed)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "\n",
    "            # Loop through DataFrame rows and insert each row into the table\n",
    "            data = []\n",
    "            for index, row in df.iterrows():\n",
    "                DATE = row['DATE'].strftime('%Y-%m-%d') \n",
    "                data.append((\n",
    "                DATE,\n",
    "                row['hour'],\n",
    "                row['HourlyPrecipitation'], row['HourlyWindSpeed']\n",
    "                ))\n",
    "        \n",
    "            with connection:\n",
    "                connection.executemany(insert_query2,data)\n",
    "\n",
    "        #add data to daily weather database       \n",
    "        if (table_name == 'daily_weather'):\n",
    "            df = table_to_df_dict[table_name]\n",
    "            insert_query3 = \"\"\"\n",
    "            INSERT INTO daily_weather(DATE,\n",
    "                                      DailyPrecipitation,\n",
    "                                      DailyAverageWindSpeed,\n",
    "                                      DailySnowfall)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "    \n",
    "            #loop through DataFrame rows and insert each row into the table\n",
    "            data = []\n",
    "            for index, row in df.iterrows():\n",
    "                DATE = row['DATE'].strftime('%Y-%m-%d') \n",
    "                data.append((\n",
    "                DATE,\n",
    "                row['DailyPrecipitation'],\n",
    "                row['DailyAverageWindSpeed'], row['DailySnowfall']\n",
    "                ))\n",
    "        \n",
    "            with connection:\n",
    "                connection.executemany(insert_query3,data)\n",
    "\n",
    "        #add data to Uber database \n",
    "        if (table_name == 'uber_trips'):\n",
    "            df = table_to_df_dict[table_name]\n",
    "            insert_query4 = \"\"\"\n",
    "            INSERT INTO uber_trips (request_datetime,\n",
    "                                    pickup_datetime,\n",
    "                                    dropoff_datetime,\n",
    "                                    trip_miles,\n",
    "                                    trip_time_seconds,\n",
    "                                    pickup_longitude,\n",
    "                                    pickup_latitude,\n",
    "                                    dropoff_longitude,\n",
    "                                    dropoff_latitude,\n",
    "                                    base_passenger_fare,\n",
    "                                    tolls,\n",
    "                                    black_car_fund,\n",
    "                                    sales_tax,\n",
    "                                    congestion_surcharge,\n",
    "                                    airport_fee,\n",
    "                                    tips,\n",
    "                                    all_surcharge)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "    \n",
    "            #loop through DataFrame rows and insert each row into the table\n",
    "            data = []\n",
    "            for index, row in df.iterrows():\n",
    "                request_datetime = row['request_datetime'].strftime('%Y-%m-%d %H:%M:%S') \n",
    "                pickup_datetime = row['pickup_datetime'].strftime('%Y-%m-%d %H:%M:%S') \n",
    "                dropoff_datetime = row['dropoff_datetime'].strftime('%Y-%m-%d %H:%M:%S') \n",
    "                data.append((request_datetime,\n",
    "                pickup_datetime, dropoff_datetime,\n",
    "                row['trip_miles'],row['trip_time_seconds'], row['pickup_longitude'],\n",
    "                row['pickup_latitude'], row['dropoff_longitude'],row['dropoff_latitude'], \n",
    "                row['base_passenger_fare'], row['tolls'], row['black_car_fund'],\n",
    "                row['sales_tax'], row['congestion_surcharge'], row['airport_fee'], \n",
    "                row['tips'], row['all_surcharge']\n",
    "                ))\n",
    "        \n",
    "            with connection:\n",
    "                connection.executemany(insert_query4,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query: str, outfile: str)-> None:\n",
    "    \"\"\"\n",
    "    Write the SQL query to a specified file.\n",
    "\n",
    "    Parameters:\n",
    "        query (str): The SQL query to be written to the file.\n",
    "        outfile (str): The path to the output file where the query should be stored.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"the_most_popular_hour_to_take_a_taxi.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT \n",
    "strftime('%H', pickup_datetime) AS hour_of_day,\n",
    "COUNT(pickup_datetime) AS number_of_taxi\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' AND pickup_datetime < '2024-09-01'\n",
    "GROUP BY hour_of_day\n",
    "ORDER BY number_of_taxi DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the result\n",
    "pd.read_sql(QUERY_1, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debeefc6",
   "metadata": {},
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ce7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"the_most_popular_day_of_the_week_to_take_an_Uber.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT \n",
    "strftime('%w', pickup_datetime) AS day_of_week,\n",
    "COUNT(pickup_datetime) AS number_of_uber\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' AND pickup_datetime < '2024-09-01'\n",
    "GROUP BY day_of_week\n",
    "ORDER BY number_of_uber DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a263732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the result\n",
    "pd.read_sql(QUERY_2, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76450bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609ea32",
   "metadata": {},
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = \"the_95_percentile_of_trip_distance_in_January_2024.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "SELECT trip_distance\n",
    "FROM (\n",
    "    SELECT trip_distance,\n",
    "    ROW_NUMBER() OVER (ORDER BY trip_distance ASC) AS row_num,\n",
    "    COUNT(*) OVER () AS total_count\n",
    "FROM (\n",
    "    SELECT trip_distance\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime >= '2024-01-01' AND pickup_datetime < '2024-02-01'\n",
    "    UNION ALL\n",
    "    SELECT trip_miles AS trip_distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime >= '2024-01-01' AND pickup_datetime < '2024-02-01') \n",
    ") \n",
    "WHERE row_num = FLOOR(total_count * 0.95)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f66d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the result\n",
    "pd.read_sql(QUERY_3, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ced4f68",
   "metadata": {},
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5205d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"the_weather_like_for_the_busiest_days_in_2023.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "SELECT ride_date, number_of_rides, average_trip_distance,\n",
    "DailyPrecipitation/24 AS Average_DailyPrecipitation ,DailyAverageWindSpeed\n",
    "FROM (\n",
    "    SELECT ride_date, \n",
    "    COUNT(*) AS number_of_rides,\n",
    "    AVG(trip_distance) AS average_trip_distance\n",
    "    FROM(\n",
    "        SELECT strftime('%Y-%m-%d', pickup_datetime) AS ride_date, trip_distance\n",
    "        FROM taxi_trips\n",
    "        WHERE pickup_datetime >= '2023-01-01' AND pickup_datetime < '2024-01-01'\n",
    "        UNION ALL\n",
    "        SELECT strftime('%Y-%m-%d', pickup_datetime) AS ride_date, trip_miles AS trip_distance\n",
    "        FROM uber_trips\n",
    "        WHERE pickup_datetime >= '2023-01-01' AND pickup_datetime < '2024-01-01')\n",
    "    GROUP BY ride_date\n",
    "    ORDER BY number_of_rides DESC\n",
    "    LIMIT 10) AS total_rides\n",
    "JOIN daily_weather AS w ON total_rides.ride_date = w.DATE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c537ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the result\n",
    "pd.read_sql(QUERY_4, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f149ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7694a",
   "metadata": {},
   "source": [
    "### Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7122a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = \"number_of_rides_were_hired_during_snow_days.sql\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "SELECT ride_date, DailySnowfall, number_of_rides\n",
    "FROM (daily_weather as w\n",
    "JOIN(\n",
    "SELECT ride_date,\n",
    "COUNT(*) AS number_of_rides\n",
    "FROM(\n",
    "SELECT strftime('%Y-%m-%d', pickup_datetime) AS ride_date\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' AND pickup_datetime < '2024-09-01'\n",
    "UNION ALL\n",
    "SELECT strftime('%Y-%m-%d', pickup_datetime) AS ride_date\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' AND pickup_datetime < '2024-09-01')\n",
    "GROUP BY ride_date) AS t on w.DATE = t.ride_date)\n",
    "ORDER BY DailySnowfall DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8d6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the result\n",
    "pd.read_sql(QUERY_5, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350b345",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f043a41",
   "metadata": {},
   "source": [
    "### Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fea0187",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6 = \"\"\"\n",
    "SELECT w.date,w.hour,\n",
    "    COALESCE(total_rides, 0) AS total_rides,\n",
    "    COALESCE(total_precipitation, 0) AS total_precipitation,\n",
    "    COALESCE(avg_wind_speed, 0) AS avg_wind_speed\n",
    "FROM (\n",
    "SELECT strftime('%Y-%m-%d', DATE) AS date,    \n",
    "    hour,  \n",
    "    SUM(HourlyPrecipitation) AS total_precipitation,\n",
    "    AVG(HourlyWindSpeed) AS avg_wind_speed\n",
    "FROM hourly_weather\n",
    "WHERE DATE >= '2023-09-25' AND DATE <= '2023-10-03'\n",
    "GROUP BY date, hour) AS w\n",
    "LEFT JOIN(\n",
    "    SELECT date, \n",
    "    hour, \n",
    "    SUM(number_of_taxi + number_of_uber) AS total_rides\n",
    "    FROM (\n",
    "    SELECT strftime('%Y-%m-%d', pickup_datetime) AS date,\n",
    "        strftime('%H', pickup_datetime) AS hour,\n",
    "        COUNT(*) AS number_of_taxi,  \n",
    "        0 AS number_of_uber     \n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime >= '2023-09-25' AND pickup_datetime <= '2023-10-03'\n",
    "    GROUP BY date, hour\n",
    "        \n",
    "    UNION ALL\n",
    "        \n",
    "    SELECT strftime('%Y-%m-%d', pickup_datetime) AS date,\n",
    "        strftime('%H', pickup_datetime) AS hour,\n",
    "        COUNT(*) AS number_of_uber,    \n",
    "        0 AS number_of_taxi\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime >= '2023-09-25' AND pickup_datetime <= '2023-10-03'\n",
    "    GROUP BY date, hour)\n",
    "    GROUP BY date, hour) AS t\n",
    "ON w.date = t.date AND w.hour = t.hour\n",
    "ORDER BY w.date, w.hour\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64793b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the result\n",
    "pd.read_sql(QUERY_6, con=connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_most_popular_hour_to_take_a_taxi(dataframe: pd.DataFrame) -> None:\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    hours = dataframe['hour_of_day']\n",
    "    taxi_count = dataframe['number_of_taxi']\n",
    "\n",
    "    axes.bar(hours, taxi_count)\n",
    "    axes.set_xlabel('Hour of Day in 24-hour Format')\n",
    "    axes.set_xticks(range(len(hours)))\n",
    "    axes.set_ylabel('Number of Taxi')\n",
    "    axes.set_title(\"Most Popular Hour to Take a Taxi\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_most_popular_hour_to_take_a_taxi() -> pd.DataFrame:\n",
    "    dataframe = pd.read_sql(QUERY_1, con=connection)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_most_popular_hour_to_take_a_taxi()\n",
    "plot_most_popular_hour_to_take_a_taxi(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f4c3f",
   "metadata": {},
   "source": [
    "### Visualization 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_AVERAGE_DISTANCE_TRAVELED_PER_MONTH_FILENAME = \"average_distance_traveled_per_month.sql\"\n",
    "\n",
    "QUERY_AVERAGE_DISTANCE_TRAVELED_PER_MONTH = \"\"\"\n",
    "SELECT \n",
    "    strftime('%m', pickup_datetime) AS month,\n",
    "    trip_distance AS distance\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' AND pickup_datetime < '2024-09-01'\n",
    "UNION ALL\n",
    "SELECT\n",
    "    strftime('%m', pickup_datetime) AS month,\n",
    "    trip_miles AS distance\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' AND pickup_datetime < '2024-09-01'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1644841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_distance_traveled_per_month(dataframe: pd.DataFrame) -> None:\n",
    "\n",
    "    grouped = dataframe.groupby('month')['distance'].mean().reset_index()\n",
    "    grouped['month'] = grouped['month'].astype(int)\n",
    "    grouped = grouped.sort_values('month')\n",
    "    \n",
    "    x = grouped['month']  \n",
    "    y = grouped['distance'] \n",
    "    dy = 0.9\n",
    "    \n",
    "    plt.errorbar(x, y, yerr=dy, fmt='.-', label='Average Distance')\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.title('Average Distance Traveled Per Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Distance')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7febe090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_average_distance_traveled_per_month() -> pd.DataFrame:\n",
    "    dataframe = pd.read_sql(QUERY_AVERAGE_DISTANCE_TRAVELED_PER_MONTH, con=connection)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cac0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_average_distance_traveled_per_month()\n",
    "plot_average_distance_traveled_per_month(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a199b99",
   "metadata": {},
   "source": [
    "### Visualization 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_LGA_FILENAME = \"the_most_popular_day_of_the_week_for_drop_offs_in_LGA.sql\"\n",
    "\n",
    "QUERY_LGA = f\"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(pickup_datetime) AS number_of_trips\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' \n",
    "    AND pickup_datetime < '2024-09-01'\n",
    "    AND dropoff_latitude BETWEEN {LGA_BOX_COORDS[0][0]} AND {LGA_BOX_COORDS[1][0]}\n",
    "    AND dropoff_longitude BETWEEN {LGA_BOX_COORDS[0][1]} AND {LGA_BOX_COORDS[1][1]}\n",
    "GROUP BY day_of_week\n",
    "UNION ALL\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(pickup_datetime) AS number_of_trips\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' \n",
    "    AND pickup_datetime < '2024-09-01'\n",
    "    AND dropoff_latitude BETWEEN {LGA_BOX_COORDS[0][0]} AND {LGA_BOX_COORDS[1][0]}\n",
    "    AND dropoff_longitude BETWEEN {LGA_BOX_COORDS[0][1]} AND {LGA_BOX_COORDS[1][1]}\n",
    "GROUP BY day_of_week\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f68fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_LGA_FILENAME = \"the_most_popular_day_of_the_week_for_drop_offs_in_JFK.sql\"\n",
    "QUERY_JFK = f\"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(pickup_datetime) AS number_of_trips\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' \n",
    "    AND pickup_datetime < '2024-09-01'\n",
    "    AND dropoff_latitude BETWEEN {JFK_BOX_COORDS[0][0]} AND {JFK_BOX_COORDS[1][0]}\n",
    "    AND dropoff_longitude BETWEEN {JFK_BOX_COORDS[0][1]} AND {JFK_BOX_COORDS[1][1]}\n",
    "GROUP BY day_of_week\n",
    "UNION ALL\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(pickup_datetime) AS number_of_trips\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' \n",
    "    AND pickup_datetime < '2024-09-01'\n",
    "    AND dropoff_latitude BETWEEN {JFK_BOX_COORDS[0][0]} AND {JFK_BOX_COORDS[1][0]}\n",
    "    AND dropoff_longitude BETWEEN {JFK_BOX_COORDS[0][1]} AND {JFK_BOX_COORDS[1][1]}\n",
    "GROUP BY day_of_week\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a569a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_LGA_FILENAME = \"the_most_popular_day_of_the_week_for_drop_offs_in_EWR.sql\"\n",
    "\n",
    "QUERY_EWR = f\"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(pickup_datetime) AS number_of_trips\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' \n",
    "    AND pickup_datetime < '2024-09-01'\n",
    "    AND dropoff_latitude BETWEEN {EWR_BOX_COORDS[0][0]} AND {EWR_BOX_COORDS[1][0]}\n",
    "    AND dropoff_longitude BETWEEN {EWR_BOX_COORDS[0][1]} AND {EWR_BOX_COORDS[1][1]}\n",
    "GROUP BY day_of_week\n",
    "UNION ALL\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(pickup_datetime) AS number_of_trips\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' \n",
    "    AND pickup_datetime < '2024-09-01'\n",
    "    AND dropoff_latitude BETWEEN {EWR_BOX_COORDS[0][0]} AND {EWR_BOX_COORDS[1][0]}\n",
    "    AND dropoff_longitude BETWEEN {EWR_BOX_COORDS[0][1]} AND {EWR_BOX_COORDS[1][1]}\n",
    "GROUP BY day_of_week\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95309fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_popular_day_for_drop_offs_in_airports(dataframe: pd.DataFrame) -> None:\n",
    "    #map day_of_week integers to day names\n",
    "    day_names = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "    dataframe['day_name'] = dataframe['day_of_week'].astype(int).apply(lambda x: day_names[x])\n",
    "\n",
    "    #group by airport and day name\n",
    "    grouped_data = dataframe.groupby(['airport', 'day_name'])['number_of_trips'].sum().reset_index()\n",
    "\n",
    "    #pivot data for easier plotting\n",
    "    pivot_data = grouped_data.pivot(index='day_name', columns='airport', values='number_of_trips').fillna(0)\n",
    "    pivot_data = pivot_data.reindex(day_names)  # Ensure days are in correct order\n",
    "\n",
    "    #plot a bar graph for each airport\n",
    "    pivot_data.plot(kind='bar', figsize=(12, 8), width=0.8)\n",
    "\n",
    "    #add titles and labels\n",
    "    plt.title('Most Popular Day of Week for Drop-Offs in Airports (LGA, JFK, EWR)', fontsize=16)\n",
    "    plt.xlabel('Day of the Week', fontsize=14)\n",
    "    plt.ylabel('Number of Trips', fontsize=14)\n",
    "    plt.xticks(rotation=45, fontsize=12)\n",
    "    plt.legend(title='Airport', fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    #show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864b39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_popular_day_for_drop_offs_in_airports() -> pd.DataFrame:\n",
    "    lga_data = pd.read_sql(QUERY_LGA, connection)\n",
    "    lga_data['airport'] = 'LGA'\n",
    "    \n",
    "    jfk_data = pd.read_sql(QUERY_JFK, connection)\n",
    "    jfk_data['airport'] = 'JFK'\n",
    "    \n",
    "    ewr_data = pd.read_sql(QUERY_EWR, connection)\n",
    "    ewr_data['airport'] = 'EWR'\n",
    "    \n",
    "    dataframe = pd.concat([lga_data, jfk_data, ewr_data])\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_popular_day_for_drop_offs_in_airports()\n",
    "plot_popular_day_for_drop_offs_in_airports(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46740016",
   "metadata": {},
   "source": [
    "### Visualization 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_MONTHLY_TOTAL_FARE_FILENAME = 'monthly_earned_total_fares_for_uber_and_taxi.sql'\n",
    "QUERY_MONTHLY_TOTAL_FARE_UBER = \"\"\"\n",
    "SELECT\n",
    "    base_passenger_fare AS base_fare,\n",
    "    all_surcharge AS all_surcharge,\n",
    "    tolls AS tolls,\n",
    "    sales_tax AS tax,\n",
    "    base_passenger_fare + all_surcharge + tolls + sales_tax AS total_fare,\n",
    "    strftime('%Y-%m', pickup_datetime) AS month_year\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' \n",
    "    AND pickup_datetime < '2024-09-01'\n",
    "\"\"\"\n",
    "QUERY_MONTHLY_TOTAL_FARE_TAXI = \"\"\"\n",
    "SELECT\n",
    "    fare_amount AS base_fare, \n",
    "    surcharge AS all_surcharge, \n",
    "    tolls_amount AS tolls, \n",
    "    taxes AS tax,\n",
    "    fare_amount + surcharge + tolls_amount + taxes AS total_fare,\n",
    "    strftime('%Y-%m', pickup_datetime) AS month_year\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' \n",
    "    AND pickup_datetime < '2024-09-01'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monthly_earned_total_fares_for_uber_and_taxi(uber_fare_data: pd.DataFrame, taxi_fare_data: pd.DataFrame) -> None:\n",
    "    #x-axis positions\n",
    "    all_months = pd.date_range(start='2020-01', end='2024-08', freq='MS')\n",
    "    months = all_months.strftime('%Y-%m')\n",
    "    x = np.arange(len(months))  \n",
    "\n",
    "    #sorted by month_year\n",
    "    uber_fare_data = uber_fare_data.groupby('month_year', as_index=False).sum()\n",
    "    taxi_fare_data = taxi_fare_data.groupby('month_year', as_index=False).sum()\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(16, 12))\n",
    "\n",
    "    #Uber stacked bar chart\n",
    "    axs[0].bar(x, uber_fare_data['base_fare'], label='Uber Base Fare', color='lightblue')\n",
    "    axs[0].bar(x, uber_fare_data['all_surcharge'], bottom=uber_fare_data['base_fare'], label='Uber Surcharge', color='orange')\n",
    "    axs[0].bar(x, uber_fare_data['tolls'], bottom=uber_fare_data['base_fare'] + uber_fare_data['all_surcharge'], label='Uber Tolls', color='green')\n",
    "    axs[0].bar(x, uber_fare_data['tax'], bottom=uber_fare_data['base_fare'] + uber_fare_data['all_surcharge'] + uber_fare_data['tolls'], label='Uber Tax', color='salmon')\n",
    "    axs[0].set_title('Monthly Earned Total Fares for Uber (2020-2024)')\n",
    "    axs[0].set_ylabel('Total Amount ($)')\n",
    "    axs[0].legend(loc='upper left')\n",
    "    axs[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    #Taxi stacked bar chart\n",
    "    axs[1].bar(x, taxi_fare_data['base_fare'], label='Taxi Base Fare', color='lightblue')\n",
    "    axs[1].bar(x, taxi_fare_data['all_surcharge'], bottom=taxi_fare_data['base_fare'], label='Taxi Surcharge', color='orange')\n",
    "    axs[1].bar(x, taxi_fare_data['tolls'], bottom=taxi_fare_data['base_fare'] + taxi_fare_data['all_surcharge'], label='Taxi Tolls', color='green')\n",
    "    axs[1].bar(x, taxi_fare_data['tax'], bottom=taxi_fare_data['base_fare'] + taxi_fare_data['all_surcharge'] + taxi_fare_data['tolls'], label='Taxi Tax', color='salmon')\n",
    "    axs[1].set_title('Monthly Earned Total Fares for Yellow Taxis (2020-2024)')\n",
    "    axs[1].set_xlabel('Month')\n",
    "    axs[1].set_ylabel('Total Amount ($)')\n",
    "    axs[1].legend(loc='upper left')\n",
    "    axs[1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    #x-axis labels for both charts\n",
    "    axs[0].set_xticks(x)\n",
    "    axs[0].set_xticklabels(months, rotation=45)\n",
    "    axs[1].set_xticks(x)\n",
    "    axs[1].set_xticklabels(months, rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639abd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_monthly_earned_total_fares_for_uber_and_taxi() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    uber_fare_data = pd.read_sql(QUERY_MONTHLY_TOTAL_FARE_UBER, con=connection)\n",
    "    taxi_fare_data = pd.read_sql(QUERY_MONTHLY_TOTAL_FARE_TAXI, con=connection)\n",
    "    \n",
    "    return uber_fare_data, taxi_fare_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e567ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_fare_data, taxi_fare_data = get_data_for_monthly_earned_total_fares_for_uber_and_taxi()\n",
    "plot_monthly_earned_total_fares_for_uber_and_taxi(uber_fare_data, taxi_fare_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf5769f",
   "metadata": {},
   "source": [
    "### Visualization 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec79fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_TAXI_TIP_FILENAME = 'amount_of_taxi_tip_affcted_by_distance_and_precipitation.sql'\n",
    "QUERY_TAXI_TIP = \"\"\"\n",
    "SELECT \n",
    "    trip_distance AS distance,\n",
    "    tip_amount AS tip,\n",
    "    pickup_datetime,\n",
    "    daily_weather.DailyPrecipitation AS precipitation\n",
    "FROM taxi_trips\n",
    "JOIN daily_weather\n",
    "    ON DATE(taxi_trips.pickup_datetime) = daily_weather.DATE\n",
    "WHERE pickup_datetime >= '2022-01-01' \n",
    "    AND pickup_datetime < '2024-01-01'\n",
    "\"\"\"\n",
    "\n",
    "QUERY_UBER_TIP_FILENAME = 'amount_of_uber_tip_affcted_by_distance_and_precipitation.sql'\n",
    "QUERY_UBER_TIP = \"\"\"\n",
    "SELECT \n",
    "    trip_miles AS distance,\n",
    "    tips AS tip,\n",
    "    pickup_datetime,\n",
    "    daily_weather.DailyPrecipitation AS precipitation\n",
    "FROM uber_trips\n",
    "JOIN daily_weather\n",
    "    ON DATE(uber_trips.pickup_datetime) = daily_weather.DATE\n",
    "WHERE pickup_datetime >= '2022-01-01' \n",
    "    AND pickup_datetime < '2024-01-01'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d180bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_amount_of_uber_tip_affcted_by_distance_and_precipitation(uber_tip_data: pd.DataFrame, taxi_tip_data: pd.DataFrame) -> None:\n",
    "    fig, axs = plt.subplots(2, 2)\n",
    "    #distance\n",
    "    axs[0, 0].scatter(taxi_data['distance'], taxi_data['tip'])\n",
    "    axs[0, 0].set_title('Yellow Taxi: Tip vs. Distance')\n",
    "    axs[0, 0].set_xlabel('Distance in miles')\n",
    "    axs[0, 0].set_ylabel('Tip')\n",
    "\n",
    "    axs[0, 1].scatter(uber_data['distance'], uber_data['tip'])\n",
    "    axs[0, 1].set_title('Uber: Tip vs. Distance')\n",
    "    axs[0, 1].set_xlabel('Distance in miles')\n",
    "    axs[0, 1].set_ylabel('Tip')\n",
    "\n",
    "    #precipitation\n",
    "    axs[1, 0].scatter(taxi_data['precipitation'], taxi_data['tip'])\n",
    "    axs[1, 0].set_title('Yellow Taxi: Tip vs. Precipitation')\n",
    "    axs[1, 0].set_xlabel('Precipitation')\n",
    "    axs[1, 0].set_ylabel('Tip')\n",
    "\n",
    "    axs[1, 1].scatter(uber_data['precipitation'], uber_data['tip'])\n",
    "    axs[1, 1].set_title('Uber: Tip vs. Precipitation')\n",
    "    axs[1, 1].set_xlabel('Precipitation')\n",
    "    axs[1, 1].set_ylabel('Tip')\n",
    "    plt.tight_layout()\n",
    "    plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_amount_of_uber_tip_affcted_by_distance_and_precipitation5() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    uber_tip_data = pd.read_sql(QUERY_UBER_TIP, con=connection)\n",
    "    taxi_tip_data = pd.read_sql(QUERY_TAXI_TIP, con=connection)\n",
    "    return uber_tip_data, taxi_tip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc95a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_tip_data, taxi_tip_data = get_data_for_amount_of_uber_tip_affcted_by_distance_and_precipitation5()\n",
    "plot_amount_of_uber_tip_affcted_by_distance_and_precipitation(uber_tip_data, taxi_tip_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e548781f",
   "metadata": {},
   "source": [
    "### Visualization 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da319d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_LOCATION_FILENAME = 'all_hired_trips_in_2020_over_map'\n",
    "QUERY_LOCATION = \"\"\"\n",
    "SELECT \n",
    "    pickup_latitude,\n",
    "    pickup_longitude,\n",
    "    dropoff_latitude,\n",
    "    dropoff_longitude\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' AND pickup_datetime < '2021-01-01'\n",
    "UNION ALL\n",
    "SELECT \n",
    "    pickup_latitude,\n",
    "    pickup_longitude,\n",
    "    dropoff_latitude,\n",
    "    dropoff_longitude\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime >= '2020-01-01' AND pickup_datetime < '2021-01-01';\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_hired_trips_in_2020_over_map(dataframe: pd.DataFrame) -> None:\n",
    "    #in the NYC area\n",
    "    dataframe = dataframe[\n",
    "        (dataframe['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) &\n",
    "        (dataframe['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]) &\n",
    "        (dataframe['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) &\n",
    "        (dataframe['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0]) &\n",
    "        (dataframe['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) &\n",
    "        (dataframe['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]) &\n",
    "        (dataframe['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1]) &\n",
    "        (dataframe['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1]) \n",
    "    ]\n",
    "    center_latitude = (NEW_YORK_BOX_COORDS[0][0] + NEW_YORK_BOX_COORDS[1][0]) / 2\n",
    "    center_longitude = (NEW_YORK_BOX_COORDS[0][1] + NEW_YORK_BOX_COORDS[1][1]) / 2\n",
    "    nyc_map = folium.Map(location=[center_latitude, center_longitude])\n",
    "    \n",
    "    pickup_locations = dataframe[['pickup_latitude', 'pickup_longitude']].values.tolist()\n",
    "    dropoff_locations = dataframe[['dropoff_latitude', 'dropoff_longitude']].values.tolist()\n",
    "    all_location = pickup_locations + dropoff_locations\n",
    "\n",
    "    HeatMap(all_location, radius = 10).add_to(nyc_map)\n",
    "    return nyc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6294c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_all_hired_trips_in_2020_over_map() -> pd.DataFrame:\n",
    "    dataframe = pd.read_sql(QUERY_LOCATION, con=connection)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_all_hired_trips_in_2020_over_map()\n",
    "plot_all_hired_trips_in_2020_over_map(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d561e",
   "metadata": {},
   "source": [
    "Annotation: Folium utilized both drop-off and pick-up locations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
